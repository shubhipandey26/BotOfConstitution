# -*- coding: utf-8 -*-
"""constituition bot code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1laf85rk-hFA2t4cZkzR7sdPZflK4GLUb

# **install the modules**
"""

!pip install llama-index
!pip install google-generativeai
!pip install pinecone-client

!pip install llama-index-llms-gemini
!pip install llama-index-embeddings-gemini
!pip install llama-index-vector-stores-pinecone



"""# **Now import the modules**"""

import os
from pinecone import Pinecone
from llama_index.llms.gemini import Gemini
from llama_index.vector_stores.pinecone import PineconeVectorStore
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core import StorageContext, VectorStoreIndex, download_loader
from llama_index.core import Settings

"""# **Give the api keys and setup llm model and embed models**"""

GOOGLE_API_KEY = "AIzaSyB8ADz4sxqRYT7GDbmiK6PHym3ETzYMx0k"
PINECONE_API_KEY = "31e1ca39-55b6-4698-9d2b-95c37511e235"

os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY
os.environ["PINECONE_API_KEY"] = PINECONE_API_KEY

llm = Gemini()
embed_model=GeminiEmbedding(model_name="models/embedding-001")
Settings.llm=llm
Settings.embed_model=embed_model
Settings.chunk_size=1024

pinecone_client = Pinecone(api_key=os.environ["PINECONE_API_KEY"])

"""# **Showing the index name and description**"""

for index in pinecone_client.list_indexes():
  print( index['name'])

index_description= pinecone_client.describe_index("knowledgeagent")
print(index_description)

"""# **load the documents**"""

from llama_index.core import SimpleDirectoryReader
documents = SimpleDirectoryReader("data").load_data()

documents

"""# **Generate the vectors for the data and storing it in pinecone for retrieval**"""

pinecone_index=pinecone_client.Index("knowledgeagent")
vector_store = PineconeVectorStore(pinecone_index=pinecone_index)
storage_context = StorageContext.from_defaults(vector_store=vector_store)
index = VectorStoreIndex.from_documents(documents,storage_context=storage_context)

chat_engine = index.as_chat_engine()
while True:
    text_input = input("User: ")
    if text_input.lower() == "exit":
        break
    try:
        response = chat_engine.chat(text_input)

        print(f"Agent: {response.response}")

    except Exception as e:
        print(f"Error during query: {str(e)}")








